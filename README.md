# MiniGPT

This is a course project on Machine Learning, a Bayesian Perspective in TU Delft EE Signal & Systems.
The code is heavily referred from https://github.com/karpathy, and is permitted by the author to use in public.

Fine-Tuning file containes the code used to fine-tune the pre-trained model.

Generated texts and images contains everythins used in the project.

MiniGPT + Bigram file contains the code and description for both models.

Tokenizer file contains regex and basic tokenizer pre-defined to use.


Project Description:

Project 6: Build Your Mini-GPT

The advent of transformer models like GPT has revolutionized Machine Learning (ML) and Deep Learning (DL). Building a mini-GPT model offers invaluable practical experience with core concepts such as attention mechanisms, language modeling, and neural network fine-tuning. This project not only solidifies a theoretical understanding but also equips students with hands-on skills that are essential in the rapidly evolving field of AI.

Steps to Build the Mini-GPT Model

Set up your Python environment and install necessary libraries like PyTorch.
Dive into the GPT architecture, focusing on its transformer mechanisms.
Implement the mini-GPT architecture with a transformer block in PyTorch.
Process your dataset for training, including tokenization and formatting.
Train your model, tuning hyperparameters for optimal performance.
Validate the modelâ€™s performance on a separate dataset.
Fine-tune the model on a chosen downstream task (optional)
Test the model to evaluate its performance on unseen data.
Document the process, architecture, and performance metrics.
